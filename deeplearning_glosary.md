# 深度学习词汇

[//]: <> http://www.wildml.com/deep-learning-glossary/

## 激活函数(Activation Function)
如果神经网络中全是线性函数，总的效果也只会是线性函数。因此，为了模拟非线性行为，必须在神经网络中使用非线性激活函数。常用的激活函数包括[sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function)，[tanh](http://mathworld.wolfram.com/HyperbolicTangent.html)，[ReLU](#relu)及相关的变体。

## Adadelta
Adadelta是一种基于梯度下降的学习算法，它允许每个参数的学习速率随时间自适应变化。Adadelta是[Adagrad](#adagrad)的一种改进，避免了对超参数过于敏感和过多地减小学习速率。Adadelta类似于[rmsprop](#rmsprop)，可用于替代平凡的[SGD](#sgd)。

## Adagrad

## Adam

## Affine Layer

## Attention Mechanism

## Alexnet

## Autoencoder

## Average-Pooling

## 反向传播 (Backpropagation)

## 时间反向传播 (Backpropagation through time, BPIT)

## 批标准化 (Batch normalization)

## 双向RNN

## Caffe

## 范畴交叉熵损失函数 (Categorical cross-entropy loss)

## 通道 (Channel)

## 卷积神经网络 (Convolutional Neural Network, CNN, CONVNET)

## 深度信念网络 (Deep Belief Network, DBN)

## Deep Dream

## Dropout

## Embedding

## Exploding Gradient Problem

## Find-Tuning

## Gradient Clipping

## Glove

## GooogleNet

## GRU

## Highway Layer

## LCML

## ILSVRC

## Inception Module

## Keras

## 长短期记忆 (Long short-term memory, LSTM)

## Max-Pooling

## MNIST

## Momentum

## 

## 多层感知机 (Multilayer Perception, MLP)

## 负对数似然 (Negative Log Likelyhood, NLL)

## 神经机器翻译 (Neural Machine Translation, NMT)

## 神经图灵机 (Neural Turing Machine, NTM)

## 非线性性 (Nonlinearity)

## 噪声对比估计 (Noise-Contrastive Estimation, NCE)

## Pooling

## 受限波尔兹曼机 (Restricted Boltzmann Mahcine, RBN)

## 循环神经网络 (Recurrent Neural Network, RNN)

## 递归神经网络 (Recursive Neural Network)

## RELU

## ResNet

## RMSProp

## seq2seq

## 随机梯度下降 (Stochastic Gradient Descent, SGD)

## Softmax

## Tensorflow

## Vanishing gradient problem

## VGG

## word2vec


